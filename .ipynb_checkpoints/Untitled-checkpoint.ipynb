{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da594944-7e8c-4685-8967-6b80e5c70925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "<meta content=\"text/html; charset=utf-8\" http-equiv=\"content-type\"/>\n",
      "<title>Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM</title>\n",
      "<!--Generated on Thu Nov 13 00:23:31 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->\n",
      "<meta content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" name=\"viewport\"/>\n",
      "<link href=\"/static/browse/0.3.4/css/arxiv-html-papers-20250916.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "<script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\"></script>\n",
      "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js\"></script>\n",
      "<script src=\"/static/browse/0.3.4/js/addons_new.js\"></script>\n",
      "<script src=\"/static/browse/0.3.4/js/feedbackOverlay.js\"></script>\n",
      "<base href=\"/html/2511.09831v1/\"/></head>\n",
      "<body>\n",
      "<nav class=\"ltx_page_navbar\">\n",
      "<nav class=\"ltx_TOC\">\n",
      "<ol class=\"ltx_toclist\">\n",
      "<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S1\" title=\"In Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">I </span><span class=\"ltx_text ltx_font_smallcaps\">Introduction</span></span></a></li>\n",
      "<li class=\"ltx_tocentry ltx_tocentry_section\">\n",
      "<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S2\" title=\"In Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">II </span><span class=\"ltx_text ltx_font_smallcaps\">Background and Related Work</span></span></a>\n",
      "<ol class=\"ltx_toclist ltx_toclist_section\">\n",
      "<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S2.SS1\" title=\"In II Background and Related Work ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">II-A </span><span class=\"ltx_text ltx_font_italic\">Recent advancements in Natural Language Processing</span></span></a></li>\n",
      "<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S2.SS2\" title=\"In II Background and Related Work ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">II-B </span><span class=\"ltx_text ltx_font_italic\">Related Work</span></span></a></li>\n",
      "</ol>\n",
      "</li>\n",
      "<li class=\"ltx_tocentry ltx_tocentry_section\">\n",
      "<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3\" title=\"In Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">III </span><span class=\"ltx_text ltx_font_smallcaps\">Methodology</span></span></a>\n",
      "<ol class=\"ltx_toclist ltx_toclist_section\">\n",
      "<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.SS1\" title=\"In III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">III-A </span><span class=\"ltx_text ltx_font_italic\">Retriever</span></span></a></li>\n",
      "<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n",
      "<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.SS2\" title=\"In III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">III-B </span><span class=\"ltx_text ltx_font_italic\">Generator</span></span></a>\n",
      "<ol class=\"ltx_toclist ltx_toclist_subsection\">\n",
      "<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.SS2.SSS0.Px1\" title=\"In III-B Generator ‣ III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\">Zero-Shot LLM generator with RAG</span></a></li>\n",
      "<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.SS2.SSS0.Px2\" title=\"In III-B Generator ‣ III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\">Finetuned LLM generator with RAG</span></a></li>\n",
      "</ol>\n",
      "</li>\n",
      "<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.SS3\" title=\"In III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">III-C </span><span class=\"ltx_text ltx_font_italic\">Embedding model</span></span></a></li>\n",
      "<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.SS4\" title=\"In III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">III-D </span><span class=\"ltx_text ltx_font_italic\">Dataset</span></span></a></li>\n",
      "<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.SS5\" title=\"In III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">III-E </span><span class=\"ltx_text ltx_font_italic\">Experimental Setting</span></span></a></li>\n",
      "<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n",
      "<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.SS6\" title=\"In III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">III-F </span><span class=\"ltx_text ltx_font_italic\">Evaluation Metrics</span></span></a>\n",
      "<ol class=\"ltx_toclist ltx_toclist_subsection\">\n",
      "<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.SS6.SSS0.Px1\" title=\"In III-F Evaluation Metrics ‣ III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\">F1-score</span></a></li>\n",
      "<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.SS6.SSS0.Px2\" title=\"In III-F Evaluation Metrics ‣ III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\">BLEU</span></a></li>\n",
      "<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.SS6.SSS0.Px3\" title=\"In III-F Evaluation Metrics ‣ III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\">Semantic Similarity</span></a></li>\n",
      "</ol>\n",
      "</li>\n",
      "</ol>\n",
      "</li>\n",
      "<li class=\"ltx_tocentry ltx_tocentry_section\">\n",
      "<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S4\" title=\"In Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">IV </span><span class=\"ltx_text ltx_font_smallcaps\">Results</span></span></a>\n",
      "<ol class=\"ltx_toclist ltx_toclist_section\">\n",
      "<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S4.SS1\" title=\"In IV Results ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">IV-A </span><span class=\"ltx_text ltx_font_italic\">Retrieval Augmented Generation</span></span></a></li>\n",
      "<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S4.SS2\" title=\"In IV Results ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">IV-B </span><span class=\"ltx_text ltx_font_italic\">Multiple Chain-of-thought reasoning</span></span></a></li>\n",
      "</ol>\n",
      "</li>\n",
      "<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S5\" title=\"In Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">V </span><span class=\"ltx_text ltx_font_smallcaps\">Discussion</span></span></a></li>\n",
      "<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S6\" title=\"In Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">VI </span><span class=\"ltx_text ltx_font_smallcaps\">Conclusion</span></span></a></li>\n",
      "</ol></nav>\n",
      "</nav>\n",
      "<div class=\"ltx_page_main\">\n",
      "<div class=\"ltx_page_content\">\n",
      "<article class=\"ltx_document ltx_authors_1line\">\n",
      "<h1 class=\"ltx_title ltx_title_document\">Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM</h1>\n",
      "<div class=\"ltx_authors\">\n",
      "<span class=\"ltx_creator ltx_role_author\">\n",
      "<span class=\"ltx_personname\">\n",
      "Neo Wang<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">1</span></sup> and Sonit Singh<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">1</span></sup>\n",
      "</span></span>\n",
      "</div>\n",
      "<div class=\"ltx_abstract\">\n",
      "<h6 class=\"ltx_title ltx_title_abstract\">Abstract</h6>\n",
      "<p class=\"ltx_p\">The course forums are increasingly significant and play vital role in facilitating student discussions and answering their questions related to the course. It provides a platform for students to post their questions related to the content and admin issues related to the course. However, there are several challenges due to the increase in the number of students enrolled in the course. The primary challenge is that students’ queries cannot be responded immediately and the instructors have to face lots of repetitive questions. To mitigate these issues, we propose a question answering system based on large language model with retrieval augmented generation (RAG) method. This work focuses on designing a question answering system with open source Large Language Model (LLM) and fine-tuning it on the relevant course dataset. To further improve the performance, we use a local knowledge base and applied RAG method to retrieve relevant documents relevant to students’ queries, where the local knowledge base contains all the course content. To mitigate the hallucination of LLMs, We also integrate it with multi chain-of-thought reasoning to overcome the challenge of hallucination in LLMs. In this work, we experiment fine-tuned LLM with RAG method on the HotpotQA dataset. The experimental results demonstrate that the fine-tuned LLM with RAG method has a strong performance on question answering task.</p>\n",
      "</div>\n",
      "<section class=\"ltx_section\" id=\"S1\">\n",
      "<h2 class=\"ltx_title ltx_title_section\">\n",
      "<span class=\"ltx_tag ltx_tag_section\">I </span><span class=\"ltx_text ltx_font_smallcaps\">Introduction</span>\n",
      "</h2>\n",
      "<div class=\"ltx_para\" id=\"S1.p1\">\n",
      "<p class=\"ltx_p\">In large university courses, online student forums (such as Moodle and Ed forum) play a crucial role in facilitating student discussions and resolving academic queries. In the beginning, it is possible for course staff to respond to queries in a timely manner. However, with a high volume of posts, many questions become repetitive, leading to delays in response times and an increased burden on instructors. Students often struggle to find existing answers, and teaching staff must repeatedly address similar concerns. Therefore, a related question recommendation system can help mitigate these challenges by efficiently suggesting previously answered relevant questions, improving student learning experiences, and reducing instructor workload.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S1.p2\">\n",
      "<p class=\"ltx_p\">In this paper, we developed a question answering system based on large language model (LLM) and retrieval augmented generation (RAG), and also fine-tuned on the relevant course dataset. The system contributes to the development of AI applications in the education domain. By introducing a related question answering system, it promotes a reduction in course staff workload. Teaching staff could focus on more complex or new questions rather than answering repetitive or simple questions. In addition, the developed system could provide an immediate response to student queries anytime (available 24/7) and reduce the need to find similar posts. The proposed system could also improve self-learning in students as they can always find help related to the course whenever they need. Additionally, leveraging LLMs for related question recommendation system could increase search precision, contributing to AI-driven educational support and broader applications such as knowledge-sharing platforms.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S1.p3\">\n",
      "<p class=\"ltx_p\">This research aims to improve the efficiency of online course forums by developing a question-answering system based on Retrieval-Augmented Generation (RAG), which leverages an external knowledge base to enhance the ability of LLM to generate more accurate and contextually relevant responses. Our system could identify whether a newly posted student question has already been answered in the course forum. If a similar question exists, the system will generate an answer based on the corresponding previous answer. Additionally, multiple chain-of-thoughts reasoning would be implemented to reduce the hallucination of LLM generator.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S1.p4\">\n",
      "<p class=\"ltx_p\">The specific objectives of this research are to:</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S1.p5\">\n",
      "<ol class=\"ltx_enumerate\" id=\"S1.I1\">\n",
      "<li class=\"ltx_item\" id=\"S1.I1.i1\" style=\"list-style-type:none;\">\n",
      "<span class=\"ltx_tag ltx_tag_item\">1.</span>\n",
      "<div class=\"ltx_para\" id=\"S1.I1.i1.p1\">\n",
      "<p class=\"ltx_p\">To design and implement RAG-based system which could help the LLM generator produce a better answer</p>\n",
      "</div>\n",
      "</li>\n",
      "<li class=\"ltx_item\" id=\"S1.I1.i2\" style=\"list-style-type:none;\">\n",
      "<span class=\"ltx_tag ltx_tag_item\">2.</span>\n",
      "<div class=\"ltx_para\" id=\"S1.I1.i2.p1\">\n",
      "<p class=\"ltx_p\">To incorporate multiple chain-of-thought (COT) reasoning to improve the performance of student question answering system</p>\n",
      "</div>\n",
      "</li>\n",
      "</ol>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S1.p6\">\n",
      "<p class=\"ltx_p\">This paper is structured as follows: The relevant background and the related work of question answering system are introduced in section  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S2\" title=\"II Background and Related Work ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>. In section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3\" title=\"III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>, we describe our approach to mitigate the current limitation. The details of experiment are provided in section<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S4\" title=\"IV Results ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> with results analysis and discussion in section<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S5\" title=\"V Discussion ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>. Finally, in section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S6\" title=\"VI Conclusion ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a>, we summarise the benefits and limitations of our proposed question answering system, including potential future directions that could further improve this work.</p>\n",
      "</div>\n",
      "</section>\n",
      "<section class=\"ltx_section\" id=\"S2\">\n",
      "<h2 class=\"ltx_title ltx_title_section\">\n",
      "<span class=\"ltx_tag ltx_tag_section\">II </span><span class=\"ltx_text ltx_font_smallcaps\">Background and Related Work</span>\n",
      "</h2>\n",
      "<div class=\"ltx_para\" id=\"S2.p1\">\n",
      "<p class=\"ltx_p\">In this section, we lay the foundation of building blocks needed to understand the literature and also based on which our proposed methodology is based on. We also provide an overview of relevant work related to question answering system.</p>\n",
      "</div>\n",
      "<section class=\"ltx_subsection\" id=\"S2.SS1\">\n",
      "<h3 class=\"ltx_title ltx_title_subsection\">\n",
      "<span class=\"ltx_tag ltx_tag_subsection\">II-A </span><span class=\"ltx_text ltx_font_italic\">Recent advancements in Natural Language Processing</span>\n",
      "</h3>\n",
      "<div class=\"ltx_para\" id=\"S2.SS1.p1\">\n",
      "<p class=\"ltx_p\">Natural Language Processing (NLP) is a sub-field of Artificial Intelligence (AI) that aims to develop computational algorithms or tools to analyse or synthesise natural language and speech. NLP spans two main tasks, namely, natural language understanding (NLU) and natural language generation (NLG). In general, the development of NLP can be divided into three generations. Early methods were rule-based, which are mostly based on rules made by humans to finish several tasks<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib14\" title=\"\">14</a>]</cite>. For example, if the input contains ’Hello’, the rules told model to respond with ’Hi’. This method could perform well on specific tasks if the rules are designed well. However, ruled-based methods require high accuracy of rules made manually and it would be expensive. Additionally, even if a model with well-designed rules performs performs good on a specific task, it is difficult to generalise well on other similar tasks. This means we need to design rules from scratch for a new tasks. To address the generalisation issue of rule-based model, the statistical models were introduced, such as N-gram<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib15\" title=\"\">15</a>]</cite>. In the stage, features could be utilised to achieve different tasks.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S2.SS1.p2\">\n",
      "<p class=\"ltx_p\">A key task in NLP is to convert human natural language into a representation that can be process by computer. One hot vector<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib16\" title=\"\">16</a>]</cite>, for example, was a popular method to represent words. It is a simple but effective method to store a word in computer. But, this method have limitations in terms of high dimensionality and also do not capture context. Every new word means we need to create a new one hot vector for it, and the size of each vector equals to the size of vocabulary. In addition, one hot vector cannot extract specific features of entities.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S2.SS1.p3\">\n",
      "<p class=\"ltx_p\">Word2Vec<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib17\" title=\"\">17</a>]</cite> utilises features to convert word to vector, and the size of vector just depends on the number of features rather than the whole vocabulary. Word2Vec learns dense representations of words, one for centre word, another for context words. Skip-gram is using centre word to predict the context word within finite context window and continuous bag of word (CBOW) is using context word to predict the centre word. GloVe<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib18\" title=\"\">18</a>]</cite> constructs the vector based on co-occurrence. It aims to reduce the dimensionality of word representation and improved the efficiency of NLP models.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S2.SS1.p4\">\n",
      "<p class=\"ltx_p\">With the development of deep learning, several neural networks, such as recurrent neural network (RNN) and Long Short-term Memory (LSTM), perform well on many NLP tasks. These models promote the application of neural network in NLP domain. But the performance is still limited by the capability of models. Therefor, self-attention mechanism is introduced.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S2.SS1.p5\">\n",
      "<p class=\"ltx_p\">Transformer model<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib7\" title=\"\">7</a>]</cite> proposed the idea of self-attention. This mechanism significantly improves results on many NLP tasks. It simulates the human attention in the structure to leverage the previous networks. The\n",
      "Bidirectional Encoder Representations from Transformers (BERT) model<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib20\" title=\"\">20</a>]</cite> proposed a novel structure based on the pre-training of transformer. Concretely, the text representations are pre-trained bi-directionally rather than single directional. This method creates various state-of-the-art models on numerous NLP tasks including Question-answering, machine-translation.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S2.SS1.p6\">\n",
      "<p class=\"ltx_p\">Building upon the Transformer architecture<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib7\" title=\"\">7</a>]</cite>, recent years have witnessed significant advancements in NLP, driven by the emergence of Large Language Models (LLMs)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib6\" title=\"\">6</a>]</cite>. These models, often consisting of billions of parameters, are trained on massive corpora and exhibit impressive capabilities in tasks such as question answering, summarisation, and language generation. Notably, LLMs like OpenAI’s GPT series are based on a decoder-only Transformer architecture<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib19\" title=\"\">19</a>]</cite>.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S2.SS1.p7\">\n",
      "<p class=\"ltx_p\">Despite their powerful generative abilities, LLMs are limited by the fixed knowledge encoded during pre-training. This has led to the development of hybrid frameworks that combine LLMs with external knowledge sources to enhance their performance and factual accuracy. One prominent approach is Retrieval-Augmented Generation (RAG)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib8\" title=\"\">8</a>]</cite>, which integrates information retrieval mechanisms into the generation process to dynamically access relevant documents at inference time. Retrieval-Augmented Generation (RAG) introduces an external retrieval mechanism to LLMs. This strategy aids LLMs to avoid hallucination and improve the performance of LLMs on generating precise content<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib8\" title=\"\">8</a>]</cite>. RAG systems consist of three main components: retriever, generator and augmentation methods. With the release of LLMs such as ChatGPT, RAG systems can be leveraged with LLMs to improve its results, and RAG system performs well on reducing hallucination of LLMs. On other hand, the LLMs with RAG could easily be updated without re-train from scratch. This means that it could be economical for its applications. Moreover, the structure of RAG indicates that it can be easily deployed locally. So there would be no concern about privacy. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S2.F1\" title=\"Figure 1 ‣ II-A Recent advancements in Natural Language Processing ‣ II Background and Related Work ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the RAG structure.</p>\n",
      "</div>\n",
      "<figure class=\"ltx_figure\" id=\"S2.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"125\" id=\"S2.F1.g1\" src=\"RAG.png\" width=\"240\"/>\n",
      "<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span>Structure of RAG system</figcaption>\n",
      "</figure>\n",
      "<div class=\"ltx_para\" id=\"S2.SS1.p8\">\n",
      "<p class=\"ltx_p\">Recently, supervised fine-tuning (SFT)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib26\" title=\"\">26</a>]</cite> is a method that adapt pre-train models on specific tasks through training them on labelled dataset. It enables the model to align with task-specific objectives, such as classification or summarisation, while retaining general language understanding. During the SFT process, we can fine-tune all parameters or partial parameters, offering a trade-off between performance and computational efficiency.</p>\n",
      "</div>\n",
      "</section>\n",
      "<section class=\"ltx_subsection\" id=\"S2.SS2\">\n",
      "<h3 class=\"ltx_title ltx_title_subsection\">\n",
      "<span class=\"ltx_tag ltx_tag_subsection\">II-B </span><span class=\"ltx_text ltx_font_italic\">Related Work</span>\n",
      "</h3>\n",
      "<div class=\"ltx_para\" id=\"S2.SS2.p1\">\n",
      "<p class=\"ltx_p\">Several studies explored the application of LLMs in education domain<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib5\" title=\"\">5</a>]</cite>. These studies focus on the Chatbot in supporting students self-learning, yet few evaluated their methods on well-designed metrics. In particular, these papers only validate their result on a few samples. RAGBench addresses this problem by proposing the TRACe evaluation framework<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib1\" title=\"\">1</a>]</cite>. The study combines twelve different datasets to evaluate RAG systems and also measures Utilisation, Relevance, Adherence, and Completeness to analyse system performance. Although the study only evaluated GTPs models which is less economical than open-source model, it provided a novel framework for evaluation the performance of RAG system.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S2.SS2.p2\">\n",
      "<p class=\"ltx_p\">In RetLLM<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib10\" title=\"\">10</a>]</cite>, authors proposed a retrieval-augmented LLM for question answering on students discussion forums. It demonstrated the effectiveness of the appropriate prompt on Question-Answering system of education domain. Moreover, prompts are designed to require that LLMs do not directly generate solutions, such as code or answer for assignment. Instead, RetLLM-E would output hints and suitable guides for students to solve their questions. The authors also combine human evaluation and ROUGE &amp; BERTScore to evaluate the model in multiple dimensions. Also, authors compared LLM with retrieval and LLM without retrieval to justify the advantages of RAG method. Preconditions are applied in RetLLM-E to mitigate hallucination. However, RetLLM-E is still limited by the quality of data source. Concretely, it might produce wrong response when there is not related documents retrieved. In addition, it performs differently on different type of questions. Although, the paper proposed a student-oriented question answering system through producing hints rather than direct answers for students for some specific question types, it lacks method to automatically evaluate the quality of hints.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S2.SS2.p3\">\n",
      "<p class=\"ltx_p\">In another study<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib2\" title=\"\">2</a>]</cite>, authors proposed RAG model on task-specific domain through combining pre-trained seq2seq model and pre-trained retriever. The RAG model outperforms state-of-the-art models on open domain question-answering tasks and approaches similar performance on other tasks such as question generation and fact verification. Moreover, it avoided several unnecessary costs including training from scratch on a new dataset, dependency on specific documents for extractive or abstractive tasks. The proposed model not only utilises the documents retrieved to improve the reliability, but also generates reliable responses with parametric memory when the corresponding resources are insufficient. The paper provides a retrieval augmented method to mitigate the hallucination. It not only highlights the reduction on hallucination, but also avoid the cost on re-training model when the dataset have to be updated. However, the study has limitations in terms of the quality of documents as inferior resources may cause bias, misleading or abuse content.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S2.SS2.p4\">\n",
      "<p class=\"ltx_p\">Li <em class=\"ltx_emph ltx_font_italic\">et al.</em><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib9\" title=\"\">9</a>]</cite> found the main challenges on deploying AI on education domain are hallucination and difficulties on updating LLMs. Thus, they introduce RAG method to improve this situation. Authors identified the high API cost if using paid LLMs such as ChatGPT and highlighted the need to explore more affordable options for applying LLMs in the education domain. Sharma <em class=\"ltx_emph ltx_font_italic\">et al.</em><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib4\" title=\"\">4</a>]</cite> proposed a novel RAG framework for domain-specific question answering task. They utilised users’ activities, such as log clicks to improve the performance of retriever. Also, they define a relevancy metric to rank the query-document during training retriever model. In terms of generator, LLM with few-shot prompt are introduced to their framework. Authors use not only the grounded sources to fine-tune the LLM, but also add some negative documents (moderately dis-similar with others) to strengthen the recall and robustness of generator. In addition, query augmentation are implemented in the framework to mitigate the ambiguous expression. Last but not least, they utilise a Named Entity Removal Module to ensure the privacy of users. The proposed method provided improvement on LLM generator to augment the question answering system, potentially mitigating the hallucination.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S2.SS2.p5\">\n",
      "<p class=\"ltx_p\">In<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib11\" title=\"\">11</a>]</cite>, the authors proposed an AI Discussion Assistant (AIDA) system to support instructors’ work and analysed the significance of AIDA in education domain. The study addresses the fact that AI tools cannot directly replace the real instructors in education domain. Thus, they use AIDA to support instructors in answers queries of student in the online forum. The AIDA offers several options including directly generating answer, retrieve previous related materials, generating response with context retrieved. The instructors could select the appropriate work mode to generate final answer. In addition, they implemented an instructor-in-the-loop method to help LLM generate a better answer. Concretely, there might be some deletions or additions based on the generation of LLM. This strategy significantly improves the quality of final answers. Also, the system is evaluated via the number of modifications, with lower modifications result in greater accuracy. However, there are still limitations of the proposed AIDA system as it does not support multimodal inputs and the relevant context retrieved is manually operated by instructor, which could be automated.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S2.SS2.p6\">\n",
      "<p class=\"ltx_p\">Boros <em class=\"ltx_emph ltx_font_italic\">et al.</em><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib12\" title=\"\">12</a>]</cite> proposed a novel recipe, <em class=\"ltx_emph ltx_font_italic\">Sherlock</em>, for question-answering task with affordable LLM. The authors firstly select several open-source LLMs as their base model including instruct model and non-instruct model. Then they perform supervised fine-tuning on two different datasets, and this yields two results. The two models from the previous step are merged with a new different model. Then a Direct Preference Optimisation is implemented on both of them. Finally, they developed two different model: one with RAG and another without RAG. Those strategies notably increase the performance of Sherlock. In addition, the final step (ablation test) demonstrates the advantages of RAG method. Sherlock achieves a great result on test set with limited parameter size. This means Sherlock successfully saves expenditure on LLM API and computational resources. However, there are limitations in their study, including model can’t correct n-grams for RAG part and experiments on specific base models.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S2.SS2.p7\">\n",
      "<p class=\"ltx_p\">Miladi <em class=\"ltx_emph ltx_font_italic\">et al.</em><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib13\" title=\"\">13</a>]</cite> focused on evaluating the effectiveness of LLM with RAG. The authors compared control group (CG) without support from AI tools and experimental group (EG) with support from AI tools. Concretely, the evaluation was finished via pre-test, post-test and System Usability Scale (SUS) questionnaire. The result demonstrated the ability of AI tools on supporting students on acquiring new knowledge and the usability of a conversational agent. However, the experiment is limited due to the small group size. Additionally, there is no ablation test to prove the effect of the RAG method. Yoran <em class=\"ltx_emph ltx_font_italic\">et al.</em><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib21\" title=\"\">21</a>]</cite> proposed a novel method - MultiChain Reasoning (MCR) which could meta-reason through multiple chains of thoughts. Firstly, the authors demonstrated the current limitations of chain-of-thought reasoning method on question answering. The result shows though the single chain-of-thought could provide useful information, it might generate wrong answer due to the incorrect direction of thoughts. In addition, this approach would take advantage of the evidence from the multiple chains to generate the final answer instead of directly aggregating them together. The authors mainly experiment this method with implicit reasoning and explicit reasoning. It turns out that their model outperforms all related previous models. Additionally, the final result is improved through combining their model with another model - self-consistency (SC)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib22\" title=\"\">22</a>]</cite> from others’ study. The study did not apply any supervised fine-tuning method to improve the performance of the model. Also, the evaluation of the explanation quality of their model was done manually which would be inefficient and costly.</p>\n",
      "</div>\n",
      "</section>\n",
      "</section>\n",
      "<section class=\"ltx_section\" id=\"S3\">\n",
      "<h2 class=\"ltx_title ltx_title_section\">\n",
      "<span class=\"ltx_tag ltx_tag_section\">III </span><span class=\"ltx_text ltx_font_smallcaps\">Methodology</span>\n",
      "</h2>\n",
      "<div class=\"ltx_para\" id=\"S3.p1\">\n",
      "<p class=\"ltx_p\">According to the literature review in the previous section, we found that various studies have explored the integration of LLM into education. However, the majority of them are simply adopting LLM API to build a question answering system. Although, this approach takes advantage of the pre-trained knowledge of the LLM, it does not take into account the specific course related knowledge, which is highly valuable and unique to particular course in a particular university. Moreover, using paid LLM APIs such as ChatGPT could be very expensive when thousands of students were using the forum and every call to the API costs. Therefore, it is important to explore alternatives, such as open-source LLMs to apply them in the education settings. Also, we observe that most of the existing work in literature suffer from hallucination problem. Finally, existing work on Question-Answering systems in education settings focused on providing the final answer, which can sometimes be deteriorating from students’ learning perspective. To overcome all the above highlighted limitations, we propose a Question-Answering system based on open-source LLM and making use of local course content using RAG method and fine-tuning it so that the system can learn content specific to the course content. Moreover, to overcome the limitation of not giving directly the final answer, we integrated multiple chain-of-thought reasoning approach. The proposed system not only is cost-effective, can provide step by step response to students’ question, but also overcomes the problem of hallucination by enhancing the capability of the LLM generator.</p>\n",
      "</div>\n",
      "<figure class=\"ltx_figure\" id=\"S3.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"441\" id=\"S3.F2.g1\" src=\"Structure.png\" width=\"880\"/>\n",
      "<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span>Multi-chains Reasoning with RAG VS Traditional Question-answering System</figcaption>\n",
      "</figure>\n",
      "<div class=\"ltx_para\" id=\"S3.p2\">\n",
      "<p class=\"ltx_p\">Our proposed finetuned RAG-enabled LLM using multiple chain-of-thought reasoning architecture for answering students’ questions on course forums is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.F2\" title=\"Figure 2 ‣ III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. We propose a method to answer students’ questions on course forum based on the past offerings of the course data. Our question answering system constructs a local knowledge base for course based on its current content and past questions and answers. When students post their questions on forum, the system would search the top-k similar documents or questions based on the input queries. Then the retrieved documents will be passed into the first LLM to generate multi-chains. The results of multi-chains are combined with the original questions and the designed prompt. The results of the multi chain-of-thought, question and the prompt, are given as the input to the second LLM (generator) and the generator will leveraging the evidence to the final answer. In summary, first the LLM is prompted to produced multiple chain-of-thought reasoning intermediate processes based on retrieved documents and the original question. After this, the reasoning steps are collected and passed to the LLM. Finally, after receiving all intermediate processes, the LLM would utilise them as evidence to generate the final answer. We illustrate the effectiveness of the multiple chain-of-thought (COT) reasoning and the entire process in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.F3\" title=\"Figure 3 ‣ III-C Embedding model ‣ III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The question (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.F3\" title=\"Figure 3 ‣ III-C Embedding model ‣ III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) seeks the information about which band a member of Mother Love Bone belonged to before his death, just prior to the release of “Apple”. To solve this query, three documents are returned by the retriever. The first document told the information about band member, the second told the previous band of this member, and the third document told the data of the release of “Apple”. Then the first LLM produced three chains to infer the final answer. The first COT generated an incorrect answer, but the remaining two both generated the correct result. Finally, the second LLM generator produced the correct answer based on the previous results. From this simplified example, we could observe that only one chain-of-thought reasoning might lead to a wrong result. But multiple chain-of-thought reasoning could avoid this situation as much as possible. In the following sections, we provide more details about various blocks of our proposed architecture.</p>\n",
      "</div>\n",
      "<section class=\"ltx_subsection\" id=\"S3.SS1\">\n",
      "<h3 class=\"ltx_title ltx_title_subsection\">\n",
      "<span class=\"ltx_tag ltx_tag_subsection\">III-A </span><span class=\"ltx_text ltx_font_italic\">Retriever</span>\n",
      "</h3>\n",
      "<div class=\"ltx_para\" id=\"S3.SS1.p1\">\n",
      "<p class=\"ltx_p\">Each document in our dataset is converted into embedding vector space. Similarly, the student questions are also expressed as vector representations. Then the retriever would search the top-k documents which have similar semantics to input queries through search function provided by FAISS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib23\" title=\"\">23</a>]</cite> library. Then the retriever would pass the retrieved documents to generator as context.</p>\n",
      "</div>\n",
      "</section>\n",
      "<section class=\"ltx_subsection\" id=\"S3.SS2\">\n",
      "<h3 class=\"ltx_title ltx_title_subsection\">\n",
      "<span class=\"ltx_tag ltx_tag_subsection\">III-B </span><span class=\"ltx_text ltx_font_italic\">Generator</span>\n",
      "</h3>\n",
      "<div class=\"ltx_para\" id=\"S3.SS2.p1\">\n",
      "<p class=\"ltx_p\">We adopted Llama-3.2-3B-Instruct<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib24\" title=\"\">24</a>]</cite> as our base model due to its excellent performance on several tasks.</p>\n",
      "</div>\n",
      "<section class=\"ltx_paragraph\" id=\"S3.SS2.SSS0.Px1\">\n",
      "<h4 class=\"ltx_title ltx_title_paragraph\">Zero-Shot LLM generator with RAG</h4>\n",
      "<div class=\"ltx_para\" id=\"S3.SS2.SSS0.Px1.p1\">\n",
      "<p class=\"ltx_p\">We design a simple prompt to ask LLM generate answer according to the retrieved documents. In our experiments, we found that a long complex prompt might negatively impact the LLM to generate the correct answer. The retrieved documents are too long for LLM to understand the whole context. Therefore, the prompt is designed simply to reduce the complexity of context.</p>\n",
      "</div>\n",
      "</section>\n",
      "<section class=\"ltx_paragraph\" id=\"S3.SS2.SSS0.Px2\">\n",
      "<h4 class=\"ltx_title ltx_title_paragraph\">Finetuned LLM generator with RAG</h4>\n",
      "<div class=\"ltx_para\" id=\"S3.SS2.SSS0.Px2.p1\">\n",
      "<p class=\"ltx_p\">To improve the performance, we implemented LoRA method to finetune the LLM generator. The loss of prompt parts is set as a small negative constant, so the model could only learn the response part.</p>\n",
      "</div>\n",
      "</section>\n",
      "</section>\n",
      "<section class=\"ltx_subsection\" id=\"S3.SS3\">\n",
      "<h3 class=\"ltx_title ltx_title_subsection\">\n",
      "<span class=\"ltx_tag ltx_tag_subsection\">III-C </span><span class=\"ltx_text ltx_font_italic\">Embedding model</span>\n",
      "</h3>\n",
      "<div class=\"ltx_para\" id=\"S3.SS3.p1\">\n",
      "<p class=\"ltx_p\">We used <span class=\"ltx_text ltx_font_italic\">all-MiniLM-L6-V2</span> as an <em class=\"ltx_emph ltx_font_italic\">embedding</em> model. It is a sentence embedding transformer-based model, and can map input sentences 384 dimensional dense vector space. The embedding model is implemented to semantically search the related documents.</p>\n",
      "</div>\n",
      "<figure class=\"ltx_figure\" id=\"S3.F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"810\" id=\"S3.F3.g1\" src=\"example_mcr.png\" width=\"1440\"/>\n",
      "<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span>Example of Multi-chains Reasoning.</figcaption>\n",
      "</figure>\n",
      "</section>\n",
      "<section class=\"ltx_subsection\" id=\"S3.SS4\">\n",
      "<h3 class=\"ltx_title ltx_title_subsection\">\n",
      "<span class=\"ltx_tag ltx_tag_subsection\">III-D </span><span class=\"ltx_text ltx_font_italic\">Dataset</span>\n",
      "</h3>\n",
      "<div class=\"ltx_para\" id=\"S3.SS4.p1\">\n",
      "<p class=\"ltx_p\">The HotpotQA dataset mainly consists of questions, answers and a set of documents for retrieval. The HotpotQA dataset<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib3\" title=\"\">3</a>]</cite> is a large-scale, comprehensive dataset designed for multi-hop question answering that requires complex reasoning over multiple retrieved documents. The documents used for the retrieval component in our system are derived from Wikipedia articles, consistent with the original dataset setup.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S3.SS4.p2\">\n",
      "<p class=\"ltx_p\">According to the analysis of previous study<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib3\" title=\"\">3</a>]</cite>, there are more than 20 different types of questions in the HotpotQA dataset. To simplify the question classes, we use the corresponding answer types to redefine the question classes. The distribution of various question types in the HotpotQA dataset is given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.T1\" title=\"TABLE I ‣ III-D Dataset ‣ III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>. The distribution of samples across the training, validation, and test subsets of the HotpotQA dataset is summarised in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.T2\" title=\"TABLE II ‣ III-D Dataset ‣ III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>.</p>\n",
      "</div>\n",
      "<figure class=\"ltx_table\" id=\"S3.T1\">\n",
      "<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE I: </span>Questions in HotpotQA</figcaption>\n",
      "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n",
      "<thead class=\"ltx_thead\">\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Question Type</th>\n",
      "<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Percentage</th>\n",
      "</tr>\n",
      "</thead>\n",
      "<tbody class=\"ltx_tbody\">\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Person</th>\n",
      "<td class=\"ltx_td ltx_align_center ltx_border_t\">30%</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Group / Org</th>\n",
      "<td class=\"ltx_td ltx_align_center\">13%</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Location</th>\n",
      "<td class=\"ltx_td ltx_align_center\">10%</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Date</th>\n",
      "<td class=\"ltx_td ltx_align_center\">9%</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Number</th>\n",
      "<td class=\"ltx_td ltx_align_center\">8%</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Artwork</th>\n",
      "<td class=\"ltx_td ltx_align_center\">8%</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Yes/No</th>\n",
      "<td class=\"ltx_td ltx_align_center\">6%</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Adjective</th>\n",
      "<td class=\"ltx_td ltx_align_center\">4%</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Event</th>\n",
      "<td class=\"ltx_td ltx_align_center\">1%</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Other proper noun</th>\n",
      "<td class=\"ltx_td ltx_align_center\">6%</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Common noun</th>\n",
      "<td class=\"ltx_td ltx_align_center ltx_border_bb\">5%</td>\n",
      "</tr>\n",
      "</tbody>\n",
      "</table>\n",
      "</figure>\n",
      "<figure class=\"ltx_table\" id=\"S3.T2\">\n",
      "<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE II: </span>HotpotQA Dataset Statistics</figcaption>\n",
      "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n",
      "<thead class=\"ltx_thead\">\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Subset</th>\n",
      "<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">#Samples</th>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">Train</th>\n",
      "<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">1,880</th>\n",
      "</tr>\n",
      "</thead>\n",
      "<tbody class=\"ltx_tbody\">\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Valid</th>\n",
      "<td class=\"ltx_td ltx_align_center\">424</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Test</th>\n",
      "<td class=\"ltx_td ltx_align_center ltx_border_bb\">390</td>\n",
      "</tr>\n",
      "</tbody>\n",
      "</table>\n",
      "</figure>\n",
      "</section>\n",
      "<section class=\"ltx_subsection\" id=\"S3.SS5\">\n",
      "<h3 class=\"ltx_title ltx_title_subsection\">\n",
      "<span class=\"ltx_tag ltx_tag_subsection\">III-E </span><span class=\"ltx_text ltx_font_italic\">Experimental Setting</span>\n",
      "</h3>\n",
      "<div class=\"ltx_para\" id=\"S3.SS5.p1\">\n",
      "<p class=\"ltx_p\">We implemented our proposed architecture in Python programming language using PyTorch deep learning framework. Experiments were conducted on a GPU cluster to accelerate computing. Various hyperparameters settings are summarised in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S3.T3\" title=\"TABLE III ‣ III-E Experimental Setting ‣ III Methodology ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>. There were a total of 1,805,760,512 parameters, with 2,293,760 trainable parameters. By implementing LoRA method, we only train the 0.127% parameters to improve the efficiency during fine-tuning.</p>\n",
      "</div>\n",
      "<figure class=\"ltx_table\" id=\"S3.T3\">\n",
      "<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE III: </span>Parameter setting</figcaption>\n",
      "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n",
      "<thead class=\"ltx_thead\">\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Parameter</th>\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Value</th>\n",
      "</tr>\n",
      "</thead>\n",
      "<tbody class=\"ltx_tbody\">\n",
      "<tr class=\"ltx_tr\">\n",
      "<td class=\"ltx_td ltx_align_left ltx_border_t\">LoRA rank</td>\n",
      "<td class=\"ltx_td ltx_align_left ltx_border_t\">8</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<td class=\"ltx_td ltx_align_left\">LoRA alpha</td>\n",
      "<td class=\"ltx_td ltx_align_left\">16</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<td class=\"ltx_td ltx_align_left\">LoRA dropout</td>\n",
      "<td class=\"ltx_td ltx_align_left\">0.05</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<td class=\"ltx_td ltx_align_left\">Learning rate</td>\n",
      "<td class=\"ltx_td ltx_align_left\">2e-9</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<td class=\"ltx_td ltx_align_left\">Training epochs</td>\n",
      "<td class=\"ltx_td ltx_align_left\">3</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<td class=\"ltx_td ltx_align_left\">Training batch size</td>\n",
      "<td class=\"ltx_td ltx_align_left\">2</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<td class=\"ltx_td ltx_align_left\">Evaluation batch size</td>\n",
      "<td class=\"ltx_td ltx_align_left\">2</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<td class=\"ltx_td ltx_align_left ltx_border_bb\">Gradient accumulation steps</td>\n",
      "<td class=\"ltx_td ltx_align_left ltx_border_bb\">4</td>\n",
      "</tr>\n",
      "</tbody>\n",
      "</table>\n",
      "</figure>\n",
      "</section>\n",
      "<section class=\"ltx_subsection\" id=\"S3.SS6\">\n",
      "<h3 class=\"ltx_title ltx_title_subsection\">\n",
      "<span class=\"ltx_tag ltx_tag_subsection\">III-F </span><span class=\"ltx_text ltx_font_italic\">Evaluation Metrics</span>\n",
      "</h3>\n",
      "<div class=\"ltx_para\" id=\"S3.SS6.p1\">\n",
      "<p class=\"ltx_p\">Following previous studies on question-answering systems, F1 score is primarily used to evaluate our model on this dataset, complemented by BLEU for fluency/overlap and Semantic Similarity for meaning preservation.</p>\n",
      "</div>\n",
      "<section class=\"ltx_paragraph\" id=\"S3.SS6.SSS0.Px1\">\n",
      "<h4 class=\"ltx_title ltx_title_paragraph\">F1-score</h4>\n",
      "<div class=\"ltx_para\" id=\"S3.SS6.SSS0.Px1.p1\">\n",
      "<p class=\"ltx_p\">F1 score is a metric used to evaluate the overall accuracy of a model by considering both precision (<math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math>) and recall (<math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math>) through their harmonic mean. In our experiments, we use the macro-average F1 score to assess our question-answering system’s performance across all instances.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S3.SS6.SSS0.Px1.p2\">\n",
      "<p class=\"ltx_p\">The formula for the F1 score is:</p>\n",
      "<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E1\">\n",
      "<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n",
      "<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"F1=2\\cdot\\frac{P\\cdot R}{P+R},\" class=\"ltx_Math\" display=\"block\" id=\"S3.E1.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mn>1</mn></mrow><mo>=</mo><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mfrac><mrow><mi>P</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mi>R</mi></mrow><mrow><mi>P</mi><mo>+</mo><mi>R</mi></mrow></mfrac></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">F1=2\\cdot\\frac{P\\cdot R}{P+R},</annotation></semantics></math></td>\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n",
      "</tr></tbody>\n",
      "</table>\n",
      "<p class=\"ltx_p\">Where Precision and Recall are defined as:</p>\n",
      "<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E2\">\n",
      "<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n",
      "<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"P=\\frac{\\text{True Positives}}{\\text{True Positives}+\\text{False Positives}},\" class=\"ltx_Math\" display=\"block\" id=\"S3.E2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo>=</mo><mfrac><mtext>True Positives</mtext><mrow><mtext>True Positives</mtext><mo>+</mo><mtext>False Positives</mtext></mrow></mfrac></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">P=\\frac{\\text{True Positives}}{\\text{True Positives}+\\text{False Positives}},</annotation></semantics></math></td>\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n",
      "</tr></tbody>\n",
      "</table>\n",
      "<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E3\">\n",
      "<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n",
      "<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"R=\\frac{\\text{True Positives}}{\\text{True Positives}+\\text{False Negatives}}.\" class=\"ltx_Math\" display=\"block\" id=\"S3.E3.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>R</mi><mo>=</mo><mfrac><mtext>True Positives</mtext><mrow><mtext>True Positives</mtext><mo>+</mo><mtext>False Negatives</mtext></mrow></mfrac></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">R=\\frac{\\text{True Positives}}{\\text{True Positives}+\\text{False Negatives}}.</annotation></semantics></math></td>\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n",
      "</tr></tbody>\n",
      "</table>\n",
      "<p class=\"ltx_p\">The macro-average F1 score is calculated by first computing the F1-score for each QA pair and then taking the average across all pairs.</p>\n",
      "</div>\n",
      "</section>\n",
      "<section class=\"ltx_paragraph\" id=\"S3.SS6.SSS0.Px2\">\n",
      "<h4 class=\"ltx_title ltx_title_paragraph\">BLEU</h4>\n",
      "<div class=\"ltx_para\" id=\"S3.SS6.SSS0.Px2.p1\">\n",
      "<p class=\"ltx_p\">Bilingual Evaluation Understudy (BLEU) score is an algorithm primarily designed for machine translation (MT) quality assessment but is frequently adopted for text generation tasks like Question Answering to measure the similarity between the model’s generated text and a set of reference texts.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S3.SS6.SSS0.Px2.p2\">\n",
      "<p class=\"ltx_p\">BLEU relies on calculating the proportion of matching <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams (typically up to <math alttext=\"N=4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">N=4</annotation></semantics></math>) between the candidate and reference texts, applying a Brevity Penalty (BP) to penalise overly short outputs.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S3.SS6.SSS0.Px2.p3\">\n",
      "<p class=\"ltx_p\">The overall BLEU score is calculated as:</p>\n",
      "<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E4\">\n",
      "<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n",
      "<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\text{BLEU}=\\text{BP}\\cdot\\exp\\left(\\sum_{n=1}^{N}w_{n}\\log P_{n}\\right),\" class=\"ltx_Math\" display=\"block\" id=\"S3.E4.m1\" intent=\":literal\"><semantics><mrow><mrow><mtext>BLEU</mtext><mo>=</mo><mrow><mtext>BP</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><munderover><mo lspace=\"0em\" movablelimits=\"false\">∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>w</mi><mi>n</mi></msub><mo lspace=\"0.167em\" rspace=\"0em\">​</mo><mrow><mi>log</mi><mo lspace=\"0.167em\">⁡</mo><msub><mi>P</mi><mi>n</mi></msub></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\text{BLEU}=\\text{BP}\\cdot\\exp\\left(\\sum_{n=1}^{N}w_{n}\\log P_{n}\\right),</annotation></semantics></math></td>\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(4)</span></td>\n",
      "</tr></tbody>\n",
      "</table>\n",
      "<p class=\"ltx_p\">Where <math alttext=\"P_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>n</mi></msub><annotation encoding=\"application/x-tex\">P_{n}</annotation></semantics></math> is the modified <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram precision, <math alttext=\"w_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>w</mi><mi>n</mi></msub><annotation encoding=\"application/x-tex\">w_{n}</annotation></semantics></math> are the weights for each <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px2.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram order, and BP is the Brevity Penalty, which severely penalises candidates shorter than the reference texts:</p>\n",
      "<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E5\">\n",
      "<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n",
      "<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\text{BP}=\\begin{cases}1&amp;\\text{if }c&gt;r\\\\\n",
      "\\exp\\left(1-r/c\\right)&amp;\\text{if }c\\leq r\\end{cases},\" class=\"ltx_Math\" display=\"block\" id=\"S3.E5.m1\" intent=\":literal\"><semantics><mrow><mrow><mtext>BP</mtext><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mn>1</mn></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mrow><mtext>if </mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>c</mi></mrow><mo>&gt;</mo><mi>r</mi></mrow></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mrow><mi>r</mi><mo>/</mo><mi>c</mi></mrow></mrow><mo>)</mo></mrow></mrow></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mrow><mtext>if </mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>c</mi></mrow><mo>≤</mo><mi>r</mi></mrow></mtd></mtr></mtable></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\text{BP}=\\begin{cases}1&amp;\\text{if }c&gt;r\\\\\n",
      "\\exp\\left(1-r/c\\right)&amp;\\text{if }c\\leq r\\end{cases},</annotation></semantics></math></td>\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(5)</span></td>\n",
      "</tr></tbody>\n",
      "</table>\n",
      "<p class=\"ltx_p\">where <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px2.p3.m5\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> is the length of the candidate sentence and <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px2.p3.m6\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math> is the effective reference length that is closest to <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px2.p3.m7\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>.</p>\n",
      "</div>\n",
      "</section>\n",
      "<section class=\"ltx_paragraph\" id=\"S3.SS6.SSS0.Px3\">\n",
      "<h4 class=\"ltx_title ltx_title_paragraph\">Semantic Similarity</h4>\n",
      "<div class=\"ltx_para\" id=\"S3.SS6.SSS0.Px3.p1\">\n",
      "<p class=\"ltx_p\">Semantic Similarity evaluates how close two text segments (the model’s generated answer and the ground-truth answer) are in meaning, rather than just word overlap. This metric is crucial for capturing paraphrases and meaning equivalence that lexical metrics might miss.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S3.SS6.SSS0.Px3.p2\">\n",
      "<p class=\"ltx_p\">Semantic Similarity is typically computed using Sentence Embeddings derived from powerful Pre-trained Language Models (PLMs) such as Sentence-BERT (SBERT). The process involves generating embedding vectors (<math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><mi>𝐀</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> and <math alttext=\"\\mathbf{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px3.p2.m2\" intent=\":literal\"><semantics><mi>𝐁</mi><annotation encoding=\"application/x-tex\">\\mathbf{B}</annotation></semantics></math>) for the answers and calculating the Cosine Similarity between them.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S3.SS6.SSS0.Px3.p3\">\n",
      "<p class=\"ltx_p\">The formula for Cosine Similarity between two <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px3.p3.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-dimensional vectors <math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px3.p3.m2\" intent=\":literal\"><semantics><mi>𝐀</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> and <math alttext=\"\\mathbf{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px3.p3.m3\" intent=\":literal\"><semantics><mi>𝐁</mi><annotation encoding=\"application/x-tex\">\\mathbf{B}</annotation></semantics></math> is:</p>\n",
      "<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E6\">\n",
      "<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n",
      "<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\text{sim}(\\mathbf{A},\\mathbf{B})=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\sum_{i=1}^{n}A_{i}B_{i}}{\\sqrt{\\sum_{i=1}^{n}A_{i}^{2}}\\sqrt{\\sum_{i=1}^{n}B_{i}^{2}}}.\" class=\"ltx_Math\" display=\"block\" id=\"S3.E6.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>𝐀</mi><mo>,</mo><mi>𝐁</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>𝐀</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mi>𝐁</mi></mrow><mrow><mrow><mo stretchy=\"false\">‖</mo><mi>𝐀</mi><mo stretchy=\"false\">‖</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">‖</mo><mi>𝐁</mi><mo stretchy=\"false\">‖</mo></mrow></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><msub><mi>A</mi><mi>i</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>B</mi><mi>i</mi></msub></mrow></mrow><mrow><msqrt><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msubsup><mi>A</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt><mo lspace=\"0em\" rspace=\"0em\">​</mo><msqrt><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msubsup><mi>B</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt></mrow></mfrac></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(\\mathbf{A},\\mathbf{B})=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\sum_{i=1}^{n}A_{i}B_{i}}{\\sqrt{\\sum_{i=1}^{n}A_{i}^{2}}\\sqrt{\\sum_{i=1}^{n}B_{i}^{2}}}.</annotation></semantics></math></td>\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(6)</span></td>\n",
      "</tr></tbody>\n",
      "</table>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S3.SS6.SSS0.Px3.p4\">\n",
      "<p class=\"ltx_p\">A higher cosine similarity value (closer to <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px3.p4.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>) indicates greater semantic alignment between the generated and reference answers.</p>\n",
      "</div>\n",
      "</section>\n",
      "</section>\n",
      "</section>\n",
      "<section class=\"ltx_section\" id=\"S4\">\n",
      "<h2 class=\"ltx_title ltx_title_section\">\n",
      "<span class=\"ltx_tag ltx_tag_section\">IV </span><span class=\"ltx_text ltx_font_smallcaps\">Results</span>\n",
      "</h2>\n",
      "<div class=\"ltx_para\" id=\"S4.p1\">\n",
      "<p class=\"ltx_p\">In this section, we provide experimental results based on the evaluation metrics on the HotpotQA dataset.</p>\n",
      "</div>\n",
      "<section class=\"ltx_subsection\" id=\"S4.SS1\">\n",
      "<h3 class=\"ltx_title ltx_title_subsection\">\n",
      "<span class=\"ltx_tag ltx_tag_subsection\">IV-A </span><span class=\"ltx_text ltx_font_italic\">Retrieval Augmented Generation</span>\n",
      "</h3>\n",
      "<div class=\"ltx_para\" id=\"S4.SS1.p1\">\n",
      "<p class=\"ltx_p\">To highlight the effectiveness of RAG method, an ablation study was conducted. The results reported in the original study on HotpotQA dataset<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib3\" title=\"\">3</a>]</cite> was considered as a baseline. The experimented results, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S4.T4\" title=\"TABLE IV ‣ IV-A Retrieval Augmented Generation ‣ IV Results ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> indicate that there is a significant increase on F1 score for Llama-3.2-3B-Instruct with RAG method. We further extended our experiments by finetuning the Llama-3.2-3B-Instruct model with and without RAG. The results as given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S4.T4\" title=\"TABLE IV ‣ IV-A Retrieval Augmented Generation ‣ IV Results ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> demonstrate significant improvement in a finetuned model with RAG.</p>\n",
      "</div>\n",
      "<figure class=\"ltx_table\" id=\"S4.T4\">\n",
      "<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE IV: </span>Result of ablation study for RAG approach</figcaption>\n",
      "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n",
      "<thead class=\"ltx_thead\">\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Model</th>\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">F1</th>\n",
      "</tr>\n",
      "</thead>\n",
      "<tbody class=\"ltx_tbody\">\n",
      "<tr class=\"ltx_tr\">\n",
      "<td class=\"ltx_td ltx_align_left ltx_border_t\">Benchmark of HotpotQA<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#bib.bib3\" title=\"\">3</a>]</cite>\n",
      "</td>\n",
      "<td class=\"ltx_td ltx_align_left ltx_border_t\">34.4</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<td class=\"ltx_td ltx_align_left\">Llama-3.2-3B-Instruct without RAG</td>\n",
      "<td class=\"ltx_td ltx_align_left\">19.0</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<td class=\"ltx_td ltx_align_left\">Llama-3.2-3B-Instruct with RAG</td>\n",
      "<td class=\"ltx_td ltx_align_left\">26.8</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<td class=\"ltx_td ltx_align_left\">Finetuned Llama-3.2-3B-Instruct without RAG</td>\n",
      "<td class=\"ltx_td ltx_align_left\">59.6</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<td class=\"ltx_td ltx_align_left ltx_border_bb\">Finetuned Llama-3.2-3B-Instruct with RAG</td>\n",
      "<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">62.2</span></td>\n",
      "</tr>\n",
      "</tbody>\n",
      "</table>\n",
      "</figure>\n",
      "</section>\n",
      "<section class=\"ltx_subsection\" id=\"S4.SS2\">\n",
      "<h3 class=\"ltx_title ltx_title_subsection\">\n",
      "<span class=\"ltx_tag ltx_tag_subsection\">IV-B </span><span class=\"ltx_text ltx_font_italic\">Multiple Chain-of-thought reasoning</span>\n",
      "</h3>\n",
      "<div class=\"ltx_para\" id=\"S4.SS2.p1\">\n",
      "<p class=\"ltx_p\">In terms of multiple chain-of-thought reasoning, we compared the performance of multiple chain-of-thoughts reasoning for different chains.</p>\n",
      "</div>\n",
      "<figure class=\"ltx_table\" id=\"S4.T5\">\n",
      "<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE V: </span>Result of Multiple Chain-of-thoughts Reasoning</figcaption>\n",
      "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n",
      "<thead class=\"ltx_thead\">\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Models</th>\n",
      "<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">F1</th>\n",
      "<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BLEU</th>\n",
      "<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Semantic Similarity</th>\n",
      "</tr>\n",
      "</thead>\n",
      "<tbody class=\"ltx_tbody\">\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MCR(1 chain)</th>\n",
      "<td class=\"ltx_td ltx_align_center ltx_border_t\">30.7</td>\n",
      "<td class=\"ltx_td ltx_align_center ltx_border_t\">12.0</td>\n",
      "<td class=\"ltx_td ltx_align_center ltx_border_t\">64.6</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MCR(2 chain)</th>\n",
      "<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">32.0</span></td>\n",
      "<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">12.8</span></td>\n",
      "<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">66.6</span></td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MCR(3 chain)</th>\n",
      "<td class=\"ltx_td ltx_align_center\">28.5</td>\n",
      "<td class=\"ltx_td ltx_align_center\">10.7</td>\n",
      "<td class=\"ltx_td ltx_align_center\">66.3</td>\n",
      "</tr>\n",
      "<tr class=\"ltx_tr\">\n",
      "<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">MCR(4 chain)</th>\n",
      "<td class=\"ltx_td ltx_align_center ltx_border_bb\">27.5</td>\n",
      "<td class=\"ltx_td ltx_align_center ltx_border_bb\">10.8</td>\n",
      "<td class=\"ltx_td ltx_align_center ltx_border_bb\">65.4</td>\n",
      "</tr>\n",
      "</tbody>\n",
      "</table>\n",
      "</figure>\n",
      "<div class=\"ltx_para\" id=\"S4.SS2.p2\">\n",
      "<p class=\"ltx_p\">To align with the previous work, we use the same retriever (FAISS) and generator (Llama-3.2-3B-Instruct). When the number of chains is set as 2, our baseline model has achieved the best performance. Compared with the result of Llama-3.2-3B-Instruct with RAG, we can observe a significant increase in F1 score. This demonstrates that the multiple chain-of-thought reasoning enhance our system to generate a better answer to question.</p>\n",
      "</div>\n",
      "</section>\n",
      "</section>\n",
      "<section class=\"ltx_section\" id=\"S5\">\n",
      "<h2 class=\"ltx_title ltx_title_section\">\n",
      "<span class=\"ltx_tag ltx_tag_section\">V </span><span class=\"ltx_text ltx_font_smallcaps\">Discussion</span>\n",
      "</h2>\n",
      "<div class=\"ltx_para\" id=\"S5.p1\">\n",
      "<p class=\"ltx_p\">Large Language Models (LLMs) have the potential to transform how we support students in their learning in a university settings. Our results demonstrated that fine-tuning RAG-enabled LLM can learn course specific content and can answer students’ questions on course forums more effectively compared to without fine-tuning and without using any course-related database. Furthermore, our experiments demonstrated the need to have multiple chain-of-thought reasoning so that the developed system can answer students’ queries by giving hints in multiple turns, without directly providing answer, which will support students’ learning.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S5.p2\">\n",
      "<p class=\"ltx_p\">As given the results section, we found that there is a significant improvement in results when we use course content and previous questions accompanied with their answers from past course offerings as a local knowledge base. Having a local knowledge base not only provide more relevant content but also provide more control in terms of answers given to students. Although pretrained LLMs can respond to students’ queries but we found that the answers given by LLM to students’ questions are very generic and do not reflect the depth and scope of the course content taught at a particular institution. Using the RAG approach for retrieving relevant document from a local knowledge base helps in providing improved answers. As given in results Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S4.T4\" title=\"TABLE IV ‣ IV-A Retrieval Augmented Generation ‣ IV Results ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>, we can see a significant increase on F1 score for Llama-3.2-3B-Instruct with RAG method. Second, although existing studies showed that LLMs can answer students’ questions but we found that providing direct answer is not good from students’ learning perspective. It would be more effective to provide step-by-step reasoning so that students can understand how we reach to the final answer. Moreover, it is important to link various evidences to reach out to the final answer. According to the results of multi chain-of-thought reasoning, (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09831v1#S4.T5\" title=\"TABLE V ‣ IV-B Multiple Chain-of-thought reasoning ‣ IV Results ‣ Answering Students’ Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>), we observed that it is not always beneficial to simply increase the number of chains. We speculate that the reason why the model perform best when the number of chains is 2 is more chains might produce more incorrect content. We observe that single chain might produce the incorrect answer. Hence, it is of importance to decide the number of chains. If we increase the number of chains, there would be a drop in their overall performance. Consequently, directly increasing the quantity is not always useful. Otherwise, this not only increases the cost of multiple COT reasoning, but also might decrease the overall performance. Therefore, deciding an appropriate setting for the number of chains is crucial to improve the multiple chain-of-thought reasoning on question-answering system. Based on our ablation study, we found that having two chains provide best results, demonstrating that more that two chains significantly increase the complexity of reasoning.</p>\n",
      "</div>\n",
      "</section>\n",
      "<section class=\"ltx_section\" id=\"S6\">\n",
      "<h2 class=\"ltx_title ltx_title_section\">\n",
      "<span class=\"ltx_tag ltx_tag_section\">VI </span><span class=\"ltx_text ltx_font_smallcaps\">Conclusion</span>\n",
      "</h2>\n",
      "<div class=\"ltx_para\" id=\"S6.p1\">\n",
      "<p class=\"ltx_p\">In this work, we proposed a novel architecture combining finetuned LLM with RAG knowledge and chain-of-thought reasoning to answer students’ questions on the discussion forums. The experiments on a public question answering dataset, HotPotQA, showed that RAG knowledge, when combined with finetuned LLM, is effective in providing better answering to students’ question on course discussion forums. The multiple chain-of-thought reasoning also enhances the quality of generated responses when the number of chains is correctly set.</p>\n",
      "</div>\n",
      "<div class=\"ltx_para\" id=\"S6.p2\">\n",
      "<p class=\"ltx_p\">In our future work, we aim to further improve the proposed system by focusing on multimodal course content as we found that most course content is not just text-based. Thus, multimodal retriever would be considered to improve our question answering system. Furthermore, we could improve the effectiveness of retriever as the correctness of the final answer is highly dependent on the quality of retrieved documents from the local knowledge base using the RAG approach. Finally, there are many novel methods such as Reinforcement Learning with Human Feedback (RHLF) that have potential to strengthen our proposed framework by bringing human-in-the-loop. We believe that having automated students’ question answering system for courses with human oversight have huge potential to support students’ learning and taking away workload from instructors.</p>\n",
      "</div>\n",
      "</section>\n",
      "<section class=\"ltx_section\" id=\"Sx1\">\n",
      "<h2 class=\"ltx_title ltx_font_smallcaps ltx_title_section\">Acknowledgement</h2>\n",
      "<div class=\"ltx_para\" id=\"Sx1.p1\">\n",
      "<p class=\"ltx_p\">This research was supported by Katana, the high performance computing facility at the University of New South Wales. The authors also acknowledge the financial support provided by the School of Computer Science and Engineering for API and cloud services used in the development of the question answering system.</p>\n",
      "</div>\n",
      "</section>\n",
      "<section class=\"ltx_bibliography\" id=\"bib\">\n",
      "<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n",
      "<ul class=\"ltx_biblist\">\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib1\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[1]</span>\n",
      "<span class=\"ltx_bibblock\"> R. Friel, M. Belyi, and A. Sanyal, “Ragbench: Explainable benchmark for retrieval-augmented generation systems,” <em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2407.11005</em>, 2024.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib2\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[2]</span>\n",
      "<span class=\"ltx_bibblock\"> P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, and others, “Retrieval-augmented generation for knowledge-intensive nlp tasks,” <em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, vol. 33, pp. 9459–9474, 2020.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib3\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[3]</span>\n",
      "<span class=\"ltx_bibblock\"> Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning, “HotpotQA: A dataset for diverse, explainable multi-hop question answering,” <em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:1809.09600</em>, 2018.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib4\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[4]</span>\n",
      "<span class=\"ltx_bibblock\"> A. Nandy, S. Sharma, S. Maddhashiya, K. Sachdeva, P. Goyal, and N. Ganguly, “Question answering over electronic devices: A new benchmark dataset and a multi-task learning based QA framework,” <em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2109.05897</em>, 2021.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib5\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[5]</span>\n",
      "<span class=\"ltx_bibblock\"> A. T. Neumann, Y. Yin, S. Sowe, S. Decker, and M. Jarke, “An llm-driven chatbot in higher education for databases and information systems,” <em class=\"ltx_emph ltx_font_italic\">IEEE Transactions on Education</em>, 2024.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib6\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[6]</span>\n",
      "<span class=\"ltx_bibblock\"> M. A. K. Raiaan, M. S. H. Mukta, K. Fatema, N. M. Fahad, S. Sakib, M. M. J. Mim, J. Ahmad, M. E. Ali, and S. Azam, “A review on large language models: Architectures, applications, taxonomies, open issues and challenges,” <em class=\"ltx_emph ltx_font_italic\">IEEE access</em>, vol. 12, pp. 26839–26874, 2024.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib7\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[7]</span>\n",
      "<span class=\"ltx_bibblock\"> A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” <em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, vol. 30, 2017.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib8\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[8]</span>\n",
      "<span class=\"ltx_bibblock\"> Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, H. Wang, and H. Wang, “Retrieval-augmented generation for large language models: A survey,” <em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2312.10997</em>, vol. 2, no. 1, 2023.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib9\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[9]</span>\n",
      "<span class=\"ltx_bibblock\"> Z. Li, Z. Wang, W. Wang, K. Hung, and H. Xie, “Retrieval-Augmented Generation for Educational Application: A Systematic Survey,” <em class=\"ltx_emph ltx_font_italic\">Computers and Education: Artificial Intelligence</em>, vol. 8, pp. 100417, Jun. 2025.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib10\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[10]</span>\n",
      "<span class=\"ltx_bibblock\"> C. Mitra, M. Miroyan, R. Jain, V. Kumud, and G. Ranade, “RetLLM-E: Retrieval-Prompt Strategy for Question-Answering on Student Discussion Forums,” <em class=\"ltx_emph ltx_font_italic\">Proc. of the AAAI Conf. on Artificial Intelligence</em>, vol. 38, no. 21, pp. 23215–23223, Mar. 2024.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib11\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[11]</span>\n",
      "<span class=\"ltx_bibblock\"> S. Qiao, P. Denny, and N. Giacaman, “Oversight in Action: Experiences with Instructor-Moderated LLM Responses in an Online Discussion Forum,” <em class=\"ltx_emph ltx_font_italic\">arXiv:2412.09048</em>, Dec. 2024.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib12\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[12]</span>\n",
      "<span class=\"ltx_bibblock\"> T. Boros, R. Chivereanu, S. Dumitrescu, and O. Purcaru, “Fine-Tuning and Retrieval Augmented Generation for Question Answering Using Affordable Large Language Models,” in <em class=\"ltx_emph ltx_font_italic\">Proc. of the Third Ukrainian Natural Language Processing Workshop (UNLP) @ LREC-COLING 2024</em>, Torino, Italia, May 2024, pp. 75–82.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib13\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[13]</span>\n",
      "<span class=\"ltx_bibblock\"> F. Miladi, V. Psyché, A. Diattara, N. El Mawas, and D. Lemire, “Evaluating a GPT-4 and Retrieval-Augmented Generation-Based Conversational Agent to Enhance Learning Experience in a MOOC:,” in <em class=\"ltx_emph ltx_font_italic\">Proc. of the 17th International Conf. on Computer Supported Education</em>, Porto, Portugal, 2025, pp. 347–354.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib14\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[14]</span>\n",
      "<span class=\"ltx_bibblock\"> M. Mnasri, “Recent Advances in Conversational NLP : Towards the Standardization of Chatbot Building,” <em class=\"ltx_emph ltx_font_italic\">arXiv:1903.09025</em>, Mar. 2019.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib15\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[15]</span>\n",
      "<span class=\"ltx_bibblock\"> W. B. Cavnar and others, “N-gram-based text categorization,” in <em class=\"ltx_emph ltx_font_italic\">Proc. of SDAIR-94, 3rd annual symposium on document analysis and information retrieval</em>, Ann Arbor, Michigan, 1994, pp. 14.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib16\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[16]</span>\n",
      "<span class=\"ltx_bibblock\"> D. Karani, “Introduction to word embedding and word2vec,” <em class=\"ltx_emph ltx_font_italic\">Towards Data Science</em>, vol. 1, 2018.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib17\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[17]</span>\n",
      "<span class=\"ltx_bibblock\"> K. W. Church, “Word2Vec,” <em class=\"ltx_emph ltx_font_italic\">Natural Language Engineering</em>, vol. 23, no. 1, pp. 155–162, 2017.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib18\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[18]</span>\n",
      "<span class=\"ltx_bibblock\"> J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for word representation,” in <em class=\"ltx_emph ltx_font_italic\">Proc. of the 2014 Conf. on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2014, pp. 1532–1543.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib19\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[19]</span>\n",
      "<span class=\"ltx_bibblock\"> A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, and others, “Improving language understanding by generative pre-training,” San Francisco, CA, USA, 2018.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib20\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[20]</span>\n",
      "<span class=\"ltx_bibblock\"> J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding,” in <em class=\"ltx_emph ltx_font_italic\">Proc. of the 2019 Conf. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, Minneapolis, Minnesota, Jun. 2019, pp. 4171–4186.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib21\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[21]</span>\n",
      "<span class=\"ltx_bibblock\"> O. Yoran, T. Wolfson, B. Bogin, U. Katz, and D. Deutch, “Answering Questions by Meta-Reasoning over Multiple Chains of Thought,” in <em class=\"ltx_emph ltx_font_italic\">Proc. of the 2023 Conf. on Empirical Methods in Natural Language Processing</em>, Singapore, Dec. 2023, pp. 5942–5966.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib22\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[22]</span>\n",
      "<span class=\"ltx_bibblock\"> X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou, “Self-consistency improves chain of thought reasoning in language models,” <em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2203.11171</em>, 2022.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib23\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[23]</span>\n",
      "<span class=\"ltx_bibblock\"> M. Douze, A. Guzhva, C. Deng, J. Johnson, and G. Szilvasy, “The Faiss Library,” <em class=\"ltx_emph ltx_font_italic\">arXiv:2401.08281</em>, Feb. 2025.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib24\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[24]</span>\n",
      "<span class=\"ltx_bibblock\"> Meta LLaMA Team, “Introducing Meta Llama 3: The most capable openly available LLM to date.” [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://ai.meta.com/blog/meta-llama-3/\" title=\"\">https://ai.meta.com/blog/meta-llama-3/</a>, 2024.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib25\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[25]</span>\n",
      "<span class=\"ltx_bibblock\"> S. Sharma, D. S. Yoon, F. Dernoncourt, D. Sultania, and K. Bagga, “Retrieval Augmented Generation for Domain-Specific Question Answering,” [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2404.14760\" title=\"\">http://arxiv.org/abs/2404.14760</a>, 2024.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "<li class=\"ltx_bibitem\" id=\"bib.bib26\">\n",
      "<span class=\"ltx_tag ltx_tag_bibitem\">[26]</span>\n",
      "<span class=\"ltx_bibblock\"> A. Pareja, N.S. Nayak, H. Wang, K. Killamsetty, S. Sudalairaj, W. Zhao, S. Han, A. Bhandwaldar, G. Xu, K. Xu, L. Han, L. Inglis, and A. Srivastava, “Unveiling the Secret Recipe: A Guide for Supervised Fine-Tuning Small LLMs”, [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://arxiv.org/abs/2412.13337\" title=\"\">https://arxiv.org/abs/2412.13337</a>, 2024.\n",
      "\n",
      "</span>\n",
      "</li>\n",
      "</ul>\n",
      "</section>\n",
      "</article>\n",
      "</div>\n",
      "<footer class=\"ltx_page_footer\">\n",
      "<div class=\"ltx_page_logo\">Generated  on Thu Nov 13 00:23:31 2025 by <a class=\"ltx_LaTeXML_logo\" href=\"http://dlmf.nist.gov/LaTeXML/\"><span style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_font_smallcaps\" style=\"position:relative; bottom:2.2pt;\">a</span>T<span class=\"ltx_font_smallcaps\" style=\"font-size:120%;position:relative; bottom:-0.2ex;\">e</span></span><span style=\"font-size:90%; position:relative; bottom:-0.2ex;\">XML</span><img alt=\"Mascot Sammy\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==\"/></a>\n",
      "</div></footer>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "load_dotenv()\n",
    "lm = dspy.LM(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "def get_paper_summary(url: str) -> str:\n",
    "    \"\"\"Get a summary of a paper from a url.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "print(get_paper_summary(\"https://arxiv.org/html/2511.09831v1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d92f0c7-892e-42c8-8a0d-83bf6b250dad",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 139\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Save to file\u001b[39;00m\n\u001b[1;32m    138\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    140\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(markdown_output)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion complete! Markdown saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ArXiv HTML to Markdown Parser\n",
    "Step-by-step parsing of arXiv HTML papers\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def parse_arxiv_html(html_content):\n",
    "    \"\"\"\n",
    "    Parse arXiv HTML and convert to markdown\n",
    "    \"\"\"\n",
    "    soup = html_content\n",
    "    markdown_lines = []\n",
    "    # Step 1: Extract title\n",
    "    title = soup.find('h1', class_='ltx_title_document')\n",
    "    if title:\n",
    "        markdown_lines.append(f\"# {title.get_text(strip=True)}\\n\")\n",
    "    \n",
    "    # Step 2: Extract authors\n",
    "    authors_div = soup.find('div', class_='ltx_authors')\n",
    "    if authors_div:\n",
    "        authors = authors_div.get_text(strip=True)\n",
    "        # Clean up superscript numbers\n",
    "        authors = re.sub(r'\\d+', '', authors)\n",
    "        markdown_lines.append(f\"**Authors:** {authors}\\n\")\n",
    "    \n",
    "    # Step 3: Extract abstract\n",
    "    abstract_div = soup.find('div', class_='ltx_abstract')\n",
    "    if abstract_div:\n",
    "        markdown_lines.append(\"## Abstract\\n\")\n",
    "        # Get the abstract text, excluding the title\n",
    "        abstract_title = abstract_div.find('h6')\n",
    "        if abstract_title:\n",
    "            abstract_title.extract()\n",
    "        abstract_text = abstract_div.get_text(strip=True)\n",
    "        markdown_lines.append(f\"{abstract_text}\\n\")\n",
    "    \n",
    "    # Step 4: Extract main content sections\n",
    "    article = soup.find('article', class_='ltx_document')\n",
    "    if article:\n",
    "        sections = article.find_all('section', class_='ltx_section')\n",
    "        \n",
    "        for section in sections:\n",
    "            # Get section header\n",
    "            section_title = section.find('h2', class_='ltx_title_section')\n",
    "            if section_title:\n",
    "                # Extract section number and title\n",
    "                title_text = section_title.get_text(strip=True)\n",
    "                markdown_lines.append(f\"\\n## {title_text}\\n\")\n",
    "            \n",
    "            # Get subsections\n",
    "            subsections = section.find_all('section', class_='ltx_subsection')\n",
    "            if subsections:\n",
    "                for subsection in subsections:\n",
    "                    subsection_title = subsection.find('h3', class_='ltx_title_subsection')\n",
    "                    if subsection_title:\n",
    "                        title_text = subsection_title.get_text(strip=True)\n",
    "                        markdown_lines.append(f\"\\n### {title_text}\\n\")\n",
    "                    \n",
    "                    # Get paragraphs in subsection\n",
    "                    paragraphs = subsection.find_all('div', class_='ltx_para')\n",
    "                    for para in paragraphs:\n",
    "                        # Skip if it contains subsection title\n",
    "                        if para.find('h3') or para.find('h4'):\n",
    "                            continue\n",
    "                        para_text = para.get_text(strip=True)\n",
    "                        if para_text:\n",
    "                            markdown_lines.append(f\"{para_text}\\n\")\n",
    "            else:\n",
    "                # Get paragraphs directly in section\n",
    "                paragraphs = section.find_all('div', class_='ltx_para', recursive=False)\n",
    "                for para in paragraphs:\n",
    "                    # Skip if contains headers\n",
    "                    if para.find(['h2', 'h3', 'h4']):\n",
    "                        continue\n",
    "                    para_text = para.get_text(strip=True)\n",
    "                    if para_text:\n",
    "                        markdown_lines.append(f\"{para_text}\\n\")\n",
    "            \n",
    "            # Get tables in section\n",
    "            tables = section.find_all('figure', class_='ltx_table')\n",
    "            for table in tables:\n",
    "                caption = table.find('figcaption')\n",
    "                if caption:\n",
    "                    markdown_lines.append(f\"\\n**{caption.get_text(strip=True)}**\\n\")\n",
    "                \n",
    "                # Simple table extraction (can be improved)\n",
    "                table_elem = table.find('table')\n",
    "                if table_elem:\n",
    "                    markdown_lines.append(\"\\n| Table content |\\n|---|\\n\")\n",
    "            \n",
    "            # Get figures in section\n",
    "            figures = section.find_all('figure', class_='ltx_figure')\n",
    "            for figure in figures:\n",
    "                caption = figure.find('figcaption')\n",
    "                if caption:\n",
    "                    markdown_lines.append(f\"\\n**{caption.get_text(strip=True)}**\\n\")\n",
    "                img = figure.find('img')\n",
    "                if img and img.get('src'):\n",
    "                    markdown_lines.append(f\"![Figure]({img['src']})\\n\")\n",
    "    \n",
    "    # Step 5: Extract bibliography\n",
    "    bibliography = soup.find('section', class_='ltx_bibliography')\n",
    "    if bibliography:\n",
    "        markdown_lines.append(\"\\n## References\\n\")\n",
    "        bib_items = bibliography.find_all('li', class_='ltx_bibitem')\n",
    "        for item in bib_items:\n",
    "            ref_text = item.get_text(strip=True)\n",
    "            # Clean up the reference\n",
    "            ref_text = re.sub(r'^\\[\\d+\\]', '', ref_text)\n",
    "            markdown_lines.append(f\"- {ref_text}\\n\")\n",
    "    \n",
    "    return '\\n'.join(markdown_lines)\n",
    "\n",
    "\n",
    "def clean_markdown(markdown_text):\n",
    "    \"\"\"\n",
    "    Clean up the markdown text\n",
    "    \"\"\"\n",
    "    # Remove multiple consecutive newlines\n",
    "    markdown_text = re.sub(r'\\n{3,}', '\\n\\n', markdown_text)\n",
    "    \n",
    "    # Clean up citations [1] format\n",
    "    markdown_text = re.sub(r'\\[(\\d+)\\]', r'[\\1]', markdown_text)\n",
    "    \n",
    "    return markdown_text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the HTML file\n",
    "    \n",
    "    # Parse and convert to markdown\n",
    "    markdown_output = parse_arxiv_html(get_paper_summary(\"https://arxiv.org/html/2511.09831v1\"))\n",
    "    markdown_output = clean_markdown(markdown_output)\n",
    "    \n",
    "    # # Save to file\n",
    "    # output_path = '/'\n",
    "    # with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    #     f.write(markdown_output)\n",
    "    \n",
    "    # print(f\"Conversion complete! Markdown saved to: {output_path}\")\n",
    "    print(f\"\\nFirst 500 characters of output:\\n{markdown_output[:500]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
